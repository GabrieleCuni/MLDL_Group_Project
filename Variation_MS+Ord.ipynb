{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variation_MS+Ord.ipynb","provenance":[{"file_id":"1d3MmNjpxB-ZdUSx77ln97y8kIJ2RTonl","timestamp":1597616443690}],"collapsed_sections":[],"authorship_tag":"ABX9TyOAda9kV/O7PWGPgeGS4ATQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Sa10PDqTkIhS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"status":"ok","timestamp":1597782194806,"user_tz":-120,"elapsed":114993,"user":{"displayName":"GMG MLDL","photoUrl":"","userId":"07893454363778055087"}},"outputId":"d8073aa1-bc97-46be-99ef-3971ee9e0850"},"source":["!apt-get install python3.5\n","#!pip3 install 'torch==0.3.1'\n","!pip3 install 'torch==0.4.0'\n","!pip3 install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Note, selecting 'libpython3.5-minimal' for regex 'python3.5'\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Collecting torch==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n","\u001b[K     |████████████████████████████████| 484.0MB 36kB/s \n","\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.6.0+cu101\n","    Uninstalling torch-1.6.0+cu101:\n","      Successfully uninstalled torch-1.6.0+cu101\n","Successfully installed torch-0.4.0\n","Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"No1TCVzQkJqh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1597782362245,"user_tz":-120,"elapsed":18937,"user":{"displayName":"GMG MLDL","photoUrl":"","userId":"07893454363778055087"}},"outputId":"c9280954-4eeb-4d8e-98e5-e87b85142a61"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/gdrive/')\n","path = './gdrive/My Drive/Ego-rnn'\n","os.chdir(path)\n","cwd = os.getcwd()\n","print('Current dir: ' + cwd)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n","Current dir: /content/gdrive/My Drive/Ego-rnn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HtqFdMiYkLD4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597803903034,"user_tz":-120,"elapsed":21535660,"user":{"displayName":"GMG MLDL","photoUrl":"","userId":"07893454363778055087"}},"outputId":"fd124fbd-50d3-4f37-8b71-c59c82b04480"},"source":["from __future__ import print_function, division\n","from objectAttentionModelConvLSTM_Variation import *\n","from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n","                                RandomHorizontalFlip)\n","from tensorboardX import SummaryWriter\n","#import makeDatasetRGB as RGB\n","from makeDatasetMMAPS_Variation  import *\n","import argparse\n","import sys\n","\n","\n","def main_run(dataset, stage, train_data_dir, val_data_dir, stage1_dict, out_dir, seqLen, trainBatchSize,\n","             valBatchSize, numEpochs, lr1, decay_factor, decay_step, memSize, regression):\n","\n","\n","    if dataset == 'gtea61':\n","        num_classes = 61\n","    elif dataset == 'gtea71':\n","      num_classes = 71\n","    elif dataset == 'gtea_gaze':\n","        num_classes = 44\n","    elif dataset == 'egtea':\n","        num_classes = 106\n","    else:\n","        print('Dataset not found')\n","        sys.exit()\n","\t\t\n","\n","    model_folder = os.path.join('./', out_dir, dataset, 'rgb_ms', 'stage'+str(stage))  # Dir for saving models and log files\n","    # Create the dir\n","    if os.path.exists(model_folder):\n","        print('Directory {} exists!'.format(model_folder))\n","        sys.exit()\n","        \n","    os.makedirs(model_folder)\n","\n","    # Log files - sono log file, direi che per ora possiamo trascurarli e magari guardarli quando runniamo!\n","    writer = SummaryWriter(model_folder)\n","    train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n","    train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n","    val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n","    val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n","\n","    train_log_loss_ms= open((model_folder + '/train_log_loss_ms.txt'), 'w')\n","    val_log_loss_ms = open((model_folder + '/val_log_loss_ms.txt'), 'w')\n","    train_log_acc_ms= open((model_folder + '/train_log_acc_ms.txt'), 'w')\n","    val_log_acc_ms = open((model_folder + '/val_log_acc_ms.txt'), 'w')\n","\n","    train_log_loss_ot = open((model_folder + '/train_log_loss_ot.txt'), 'w')\n","    val_log_loss_ot = open((model_folder + '/val_log_loss_ot.txt'), 'w')\n","    train_log_acc_ot = open((model_folder + '/train_log_acc_ot.txt'), 'w')\n","    val_log_acc_ot = open((model_folder + '/val_log_acc_ot.txt'), 'w')\n","\n","    # Data loader\n","    #normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    #spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n","    #                             ToTensor(), normalize])\n","    spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224)])\n","\n","    vid_seq_train = makeDataset(train_data_dir, spatial_transform=spatial_transform, seqLen=seqLen, fmt='.png', regression=regression)\n","\n","    train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize,\n","                            shuffle=True, num_workers=4, pin_memory=True)\n","    \n","    if val_data_dir is not None:\n","        vid_seq_val = makeDataset(val_data_dir, train=False,\n","                                   spatial_transform=Compose([Scale(256), CenterCrop(224)]), #ToTensor(), normalize]),\n","                                   seqLen=seqLen, fmt='.png', regression=regression)\n","        \n","        val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize,\n","                                shuffle=False, num_workers=2, pin_memory=True)\n","        valInstances = vid_seq_val.__len__()\n","\n","\n","    trainInstances = vid_seq_train.__len__()\n","    print(f'Train instances: {trainInstances}')\n","\t\n","    train_params = []\n","    if stage == 1:\n","\n","        model = attentionModel(num_classes=num_classes, mem_size=memSize, regression=regression, seqLen=seqLen)\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","    else:\n","        model = attentionModel(num_classes=num_classes, mem_size=memSize, regression=regression, seqLen=seqLen)\n","        model.load_state_dict(torch.load(stage1_dict))\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","        #\n","        for params in model.resNet.layer4[0].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[0].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[2].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.layer4[2].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.fc.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        #if MSTask == True :\n","        for params in model.convMS.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        for params in model.MSfc.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        if regression == False:\n","            for params in model.m.parameters():\n","                params.requires_grad = True\n","                train_params += [params]\n","            for params in model.orderPredictionNetwork.parameters():\n","                params.requires_grad = True\n","                train_params += [params]\n","\n","        model.resNet.layer4[0].conv1.train(True)\n","        model.resNet.layer4[0].conv2.train(True)\n","        model.resNet.layer4[1].conv1.train(True)\n","        model.resNet.layer4[1].conv2.train(True)\n","        model.resNet.layer4[2].conv1.train(True)\n","        model.resNet.layer4[2].conv2.train(True)\n","        model.resNet.fc.train(True)\n","\n","        model.convMS.train(True)\n","        model.MSfc.train(True)\n","        if regression == False:\n","            model.m.train(True)\n","            model.orderPredictionNetwork.train(True)\n","\n","    for params in model.lstm_cell.parameters():\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","    for params in model.classifier.parameters():\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","\n","    model.lstm_cell.train(True)\n","\n","    model.classifier.train(True)\n","    model.cuda()\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    loss_pretext = nn.MSELoss()\n","\n","    optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n","\n","    optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step,\n","                                                           gamma=decay_factor)\n","\n","\t  #TRAINING\n","    train_iter = 0\n","    min_accuracy = 0\n","\n","    for epoch in range(numEpochs):\n","        optim_scheduler.step()\n","        epoch_loss = 0\n","        epoch_loss_ms = 0\n","        epoch_loss_ot = 0\n","        numCorrTrain = 0\n","        numCorrTrain_ms = 0\n","        numCorrTrain_ot = 0\n","        trainSamples = 0\n","        iterPerEpoch = 0\n","        model.lstm_cell.train(True)\n","        model.classifier.train(True)\n","        writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n","        if stage == 2:\n","            model.resNet.layer4[0].conv1.train(True)\n","            model.resNet.layer4[0].conv2.train(True)\n","            model.resNet.layer4[1].conv1.train(True)\n","            model.resNet.layer4[1].conv2.train(True)\n","            model.resNet.layer4[2].conv1.train(True)\n","            model.resNet.layer4[2].conv2.train(True)\n","            model.resNet.fc.train(True)\n","            model.resNet.fc.train(True)\n","            #if MSTask == True:\n","            model.convMS.train(True)\n","            model.MSfc.train(True)\n","            if regression == False:\n","                model.m.train(True)\n","                model.orderPredictionNetwork.train(True)\n","        for i, (inputs, inputs_maps, targets, idxPerm) in enumerate(train_loader):\n","            train_iter += 1\n","            iterPerEpoch += 1\n","            optimizer_fn.zero_grad()\n","            #labelMapsVariable = Variable(labels_maps.cuda())\n","            inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda())\n","            labelVariable = Variable(targets.cuda())\n","            orderVariable = Variable(torch.squeeze(idxPerm).cuda())\n","            trainSamples += inputs.size(0)\n","            output_label, _, ms_labels, order_pred = model(inputVariable, regression, orderVariable)\n","            loss = loss_fn(output_label, labelVariable)\n","            if stage == 2:\n","                loss.backward(retain_graph=True)\n","            else:\n","                loss.backward()\n","            if regression == False:\n","                inputMapsVariable = Variable(inputs_maps.permute(1, 0, 2, 3, 4).type(torch.LongTensor).cuda())\n","                ms_labels = ms_labels.view(-1, 2)\n","            else:\n","                inputMapsVariable = Variable(inputs_maps.permute(1, 0, 2, 3, 4).cuda())\n","                ms_labels = ms_labels.view(-1)\n","            inputMapsVariable = inputMapsVariable.contiguous().view(-1)\n","            if stage == 2:\n","                if regression == False:\n","                    loss_ms = loss_fn(ms_labels, inputMapsVariable)\n","                    _, predicted = torch.max(ms_labels.data, 1)\n","                    #numCorrTrain_ms += (predicted == targets.cuda()).sum()\n","                    #numCorrTrain_ms += torch.sum(predicted == inputMapsVariable.data).data.item()\n","                    numCorrTrain_ms += torch.sum(predicted == inputMapsVariable.data)\n","                    loss_ot = loss_fn(order_pred, orderVariable)\n","                    _, predicted = torch.max(order_pred.data, 1)\n","                    numCorrTrain_ot += torch.sum(predicted == orderVariable.data).data.item()\n","                    final_loss = loss_ms + loss_ot\n","                    epoch_loss_ms += loss_ms.data[0]\n","                    epoch_loss_ot += loss_ot.data[0]\n","                else:\n","                    final_loss = loss_pretext(ms_labels, inputMapsVariable)\n","                    epoch_loss_ms += loss_ms.data[0]\n","                final_loss.backward()\n","            optimizer_fn.step()\n","            _, predicted = torch.max(output_label.data, 1)\n","            #numCorrTrain += (predicted == targets.cuda()).sum()\n","            numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n","            epoch_loss += loss.data[0]\n","        avg_loss = epoch_loss/iterPerEpoch\n","        if stage == 2:\n","            #if MSTask == True:\n","            avg_loss_ms = epoch_loss_ms/iterPerEpoch\n","            if regression == False:\n","              avg_loss_ot = epoch_loss_ot/iterPerEpoch\n","              train_log_loss_ot.write('Train Loss Order Task after {} epochs = {}\\n'.format(epoch + 1, avg_loss_ot))\n","              trainAccuracy_ot = (numCorrTrain_ot / trainSamples) * 100\n","              train_log_acc_ot.write('Train Accuracy Order Task after {} epochs = {}\\n'.format(epoch + 1, trainAccuracy_ot))\n","            train_log_loss_ms.write('Train Loss MS after {} epochs = {}\\n'.format(epoch + 1, avg_loss_ms))\n","            trainAccuracy_ms = (numCorrTrain_ms / trainSamples) * 100\n","            train_log_acc_ms.write('Train Accuracy MS after {} epochs = {}\\n'.format(epoch + 1, trainAccuracy_ms))\n","        trainAccuracy = (numCorrTrain / trainSamples) * 100\n","        #if stage == 2 and MSTask == True:\n","        if stage == 2:\n","            if regression == True:\n","                print('Train: Epoch = {} | Loss = {} | MS_Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, avg_loss_ms, trainAccuracy))\n","            else:\n","                print('Train: Epoch = {} | Loss = {} | MS_Loss = {} | OrderTask Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, avg_loss_ms, avg_loss_ot, trainAccuracy))\n","        else:\n","            print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n","\n","        writer.add_scalar('train/epoch_loss', avg_loss, epoch+1)\n","        writer.add_scalar('train/accuracy', trainAccuracy, epoch+1)\n","\n","        if stage == 2:\n","            writer.add_scalar('train/epoch_MS_loss', avg_loss_ms, epoch+1)\n","        if val_data_dir is not None:\n","            if (epoch+1) % 1 == 0: #???\n","                model.train(False)\n","                val_loss_epoch = 0\n","                val_loss_epoch_ms = 0\n","                val_loss_epoch_ot = 0\n","                val_iter = 0\n","                val_samples = 0\n","                numCorr = 0\n","                numCorr_ms = 0\n","                numCorr_ot = 0\n","                for j, (inputs, inputs_maps, targets, idxPerm) in enumerate(val_loader):\n","                    val_iter += 1\n","                    val_samples += inputs.size(0)\n","                    #labelMapsVariable = Variable(labels_maps.cuda())\n","                    inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).cuda(), volatile=True)\n","                    labelVariable = Variable(targets.cuda(async=True), volatile=True)\n","                    orderVariable = Variable(torch.squeeze(idxPerm).cuda(), volatile = True)\n","                    output_label, _, ms_labels, order_pred = model(inputVariable, regression, orderVariable)\n","                    #output_label, _, ms_labels = model(inputVariable, regression)\n","                    val_loss = loss_fn(output_label, labelVariable) \n","                    val_loss_epoch += val_loss.data[0]\n","                    if regression == False:\n","                        inputMapsVariable = Variable(inputs_maps.permute(1, 0, 2, 3, 4).type(torch.LongTensor).cuda(), volatile=True)\n","                        ms_labels = ms_labels.view(-1,2)\n","                    else:\n","                        inputMapsVariable = Variable(inputs_maps.permute(1, 0, 2, 3, 4).cuda(), volatile=True)\n","                        ms_labels = ms_labels.view(-1)\n","                    inputMapsVariable = inputMapsVariable.contiguous().view(-1)\n","                    if stage == 2:\n","                        if regression == True:\n","                            val_loss_ms = loss_pretext(ms_labels, inputMapsVariable)\n","                        else:\n","                            val_loss_ms = loss_fn(ms_labels, inputMapsVariable)\n","                            _, predicted = torch.max(ms_labels.data, 1)\n","                            numCorr_ms += torch.sum(predicted == inputMapsVariable.data)\n","                            val_loss_ot = loss_fn(order_pred, orderVariable)\n","                            _, predicted = torch.max(order_pred.data, 1)\n","                            numCorr_ot += torch.sum(predicted == orderVariable.data).data.item()\n","                        val_loss_epoch_ms += val_loss_ms.data[0]\n","                        val_loss_epoch_ot += val_loss_ot.data[0]\n","                    _, predicted = torch.max(output_label.data, 1)\n","                    #numCorr += (predicted == targets.cuda()).sum()\n","                    numCorr += torch.sum(predicted == labelVariable.data).data.item() \n","                    '''else:\n","                        val_loss_ms = loss_fn(order_pred, orderVariable)\n","                        _, predicted = torch.max(order_pred.data, 1)\n","                        numCorr_ms += torch.sum(predicted == orderVariable.data).data.item()                           \n","                        #numCorr_ms += torch.sum(predicted == inputMapsVariable.data).data.item()'''\n","                val_accuracy = (numCorr / val_samples) * 100\n","                avg_val_loss = val_loss_epoch / val_iter\n","                if stage == 2:\n","                    avg_val_loss_ms = val_loss_epoch_ms / val_iter\n","                    val_log_loss_ms.write('Val Loss MS after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss_ms))\n","                    avg_val_loss_ot = val_loss_epoch_ot / val_iter\n","                    val_log_loss_ot.write('Val Loss Order Task after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss_ot))\n","                    if regression == False:\n","                        val_accuracy_ms = (numCorr_ms / val_samples) * 100\n","                        val_log_acc_ms.write('Val Accuracy MS after {} epochs = {}\\n'.format(epoch + 1, val_accuracy_ms))\n","                        val_accuracy_ot = (numCorr_ot / val_samples) * 100\n","                        val_log_acc_ot.write('Val Accuracy Order Task after {} epochs = {}\\n'.format(epoch + 1, val_accuracy_ot))\n","                if stage == 2:\n","                    print('Val: Epoch = {} | Loss = {} | MS_Loss = {} | Order Task Loss = {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, avg_val_loss_ms, avg_val_loss_ot, val_accuracy))\n","                else:\n","                    print('Val: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","                writer.add_scalar('val/epoch_loss', avg_val_loss, epoch + 1)\n","                writer.add_scalar('val/accuracy', val_accuracy, epoch + 1)\n","                if stage == 2:\n","                    writer.add_scalar('val/epoch_MS_loss', avg_val_loss_ms, epoch + 1)\n","                    writer.add_scalar('val/epoch_OT_loss', avg_val_loss_ot, epoch + 1)\n","                val_log_loss.write('Val Loss after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss))\n","                val_log_acc.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))\n","\t\t\t\t        #CHECK!!!\n","                if val_accuracy > min_accuracy:\n","                    save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n","                    torch.save(model.state_dict(), save_path_model)\n","                    min_accuracy = val_accuracy\n","            else:\n","                if (epoch+1) % 10 == 0:\n","                    save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n","                    torch.save(model.state_dict(), save_path_model)\n","\n","    train_log_loss.close()\n","    train_log_acc.close()\n","    val_log_acc.close()\n","    val_log_loss.close()\n","    train_log_loss_ms.close()\n","    val_log_loss_ms.close()\n","    train_log_acc_ms.close()\n","    val_log_acc_ms.close()\n","    train_log_loss_ot.close()\n","    val_log_loss_ot.close()\n","    train_log_acc_ot.close()\n","    val_log_acc_ot.close()\n","    writer.export_scalars_to_json(model_folder + \"/all_scalars.json\")\n","    writer.close()\n","\n","\n","def __main__():\n","    \n","    parser = argparse.ArgumentParser()\n","\n","    #STAGE 1\n","    \n","    parser.add_argument('--dataset', type=str, default='gtea61', help='Dataset')\n","    parser.add_argument('--stage', type=int, default=1, help='Training stage')\n","    parser.add_argument('--trainDatasetDir', type=str, default='./GTEA61', help='Train set directory')\n","    parser.add_argument('--valDatasetDir', type=str, default='./GTEA61', help='Val set directory')\n","    parser.add_argument('--outDir', type=str, default='Variation_16frames', help='Directory to save results')\n","    parser.add_argument('--stage1Dict', type=str, default='./Variation_16frames/gtea61/rgb/stage1/model_rgb_state_dict.pth', help='Stage 1 model path')\n","    parser.add_argument('--seqLen', type=int, default=16, help='Length of sequence')\n","    parser.add_argument('--trainBatchSize', type=int, default=32, help='Training batch size')\n","    parser.add_argument('--valBatchSize', type=int, default=64, help='Validation batch size')\n","    parser.add_argument('--numEpochs', type=int, default=300, help='Number of epochs')\n","    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n","    parser.add_argument('--stepSize', type=float, default=[25, 75, 150], nargs=\"+\", help='Learning rate decay step')\n","    parser.add_argument('--decayRate', type=float, default=0.1, help='Learning rate decay rate')\n","    parser.add_argument('--memSize', type=int, default=512, help='ConvLSTM hidden state size')\n","    parser.add_argument('--regressionFlag', type=int, default=False, help='Flag for regression in self-supervised task')\n","\n","    #STAGE 2\n","    '''\n","    parser.add_argument('--dataset', type=str, default='gtea61', help='Dataset')\n","    parser.add_argument('--stage', type=int, default=2, help='Training stage')\n","    parser.add_argument('--trainDatasetDir', type=str, default='./GTEA61',\n","                        help='Train set directory')\n","    parser.add_argument('--valDatasetDir', type=str, default='./GTEA61',\n","                        help='Val set directory')\n","    parser.add_argument('--outDir', type=str, default='Variation_16frames', help='Directory to save results')\n","    parser.add_argument('--stage1Dict', type=str, default='./Variation_16frames/gtea61/rgb_ms/stage1/model_rgb_state_dict.pth',\n","                        help='Stage 1 model path')\n","    parser.add_argument('--seqLen', type=int, default=16, help='Length of sequence')\n","    parser.add_argument('--trainBatchSize', type=int, default=32, help='Training batch size')\n","    parser.add_argument('--valBatchSize', type=int, default=64, help='Validation batch size')\n","    parser.add_argument('--numEpochs', type=int, default=150, help='Number of epochs')\n","    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n","    parser.add_argument('--stepSize', type=float, default=[25, 75], nargs=\"+\", help='Learning rate decay step')\n","    parser.add_argument('--decayRate', type=float, default=0.1, help='Learning rate decay rate')\n","    parser.add_argument('--memSize', type=int, default=512, help='ConvLSTM hidden state size')\n","    parser.add_argument('--regressionFlag', type=bool, default=False, help='Flag for regression in self-supervised task')\n","    '''\n","\n","    #args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","\n","    dataset = args.dataset\n","    stage = args.stage\n","    trainDatasetDir = args.trainDatasetDir\n","    valDatasetDir = args.valDatasetDir\n","    outDir = args.outDir\n","    stage1Dict = args.stage1Dict\n","    seqLen = args.seqLen\n","    trainBatchSize = args.trainBatchSize\n","    valBatchSize = args.valBatchSize\n","    numEpochs = args.numEpochs\n","    lr1 = args.lr\n","    stepSize = args.stepSize\n","    decayRate = args.decayRate\n","    memSize = args.memSize\n","    regression = args.regressionFlag\n","    #MSTask = args.MSflag\n","\n","    main_run(dataset, stage, trainDatasetDir, valDatasetDir, stage1Dict, outDir, seqLen, trainBatchSize, valBatchSize, numEpochs, lr1, decayRate, stepSize, memSize, regression)\n","\n","__main__()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train instances: 341\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.torch/models/resnet34-333f7ec4.pth\n","100%|██████████| 87306240/87306240 [00:02<00:00, 42653981.71it/s]\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_xx.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:32: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_i_xx.bias, 0)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:33: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_hh.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:35: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_xx.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:36: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_f_xx.bias, 0)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:37: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_hh.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:39: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_xx.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_c_xx.bias, 0)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:41: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_hh.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:43: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_xx.weight)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:44: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_o_xx.bias, 0)\n","/content/gdrive/My Drive/Ego-rnn/MyConvLSTMCell.py:45: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_hh.weight)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:286: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"],"name":"stderr"},{"output_type":"stream","text":["Train: Epoch = 1 | Loss = 4.102425575256348 | Accuracy = 4.398826979472141\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:329: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:330: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:331: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:335: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:338: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Val: Epoch = 1 | Loss = 4.069135665893555 | Accuracy = 6.0344827586206895\n","Train: Epoch = 2 | Loss = 4.057037353515625 | Accuracy = 6.158357771260997\n","Val: Epoch = 2 | Loss = 3.9569907188415527 | Accuracy = 3.4482758620689653\n","Train: Epoch = 3 | Loss = 4.0105814933776855 | Accuracy = 4.105571847507331\n","Val: Epoch = 3 | Loss = 3.9280314445495605 | Accuracy = 6.0344827586206895\n","Train: Epoch = 4 | Loss = 4.018466949462891 | Accuracy = 5.865102639296188\n","Val: Epoch = 4 | Loss = 3.9678850173950195 | Accuracy = 6.896551724137931\n","Train: Epoch = 5 | Loss = 3.9893248081207275 | Accuracy = 6.451612903225806\n","Val: Epoch = 5 | Loss = 3.9039340019226074 | Accuracy = 6.0344827586206895\n","Train: Epoch = 6 | Loss = 3.8906803131103516 | Accuracy = 7.331378299120235\n","Val: Epoch = 6 | Loss = 3.8196234703063965 | Accuracy = 6.0344827586206895\n","Train: Epoch = 7 | Loss = 3.8744239807128906 | Accuracy = 8.504398826979472\n","Val: Epoch = 7 | Loss = 3.915343761444092 | Accuracy = 6.0344827586206895\n","Train: Epoch = 8 | Loss = 3.761324167251587 | Accuracy = 9.090909090909092\n","Val: Epoch = 8 | Loss = 3.7263283729553223 | Accuracy = 6.0344827586206895\n","Train: Epoch = 9 | Loss = 3.8382670879364014 | Accuracy = 6.744868035190615\n","Val: Epoch = 9 | Loss = 3.7767884731292725 | Accuracy = 9.482758620689655\n","Train: Epoch = 10 | Loss = 3.773818016052246 | Accuracy = 9.090909090909092\n","Val: Epoch = 10 | Loss = 3.7037346363067627 | Accuracy = 10.344827586206897\n","Train: Epoch = 11 | Loss = 3.730379343032837 | Accuracy = 10.263929618768328\n","Val: Epoch = 11 | Loss = 3.611288547515869 | Accuracy = 12.068965517241379\n","Train: Epoch = 12 | Loss = 3.665111541748047 | Accuracy = 10.263929618768328\n","Val: Epoch = 12 | Loss = 3.527672290802002 | Accuracy = 7.758620689655173\n","Train: Epoch = 13 | Loss = 3.6086246967315674 | Accuracy = 14.369501466275661\n","Val: Epoch = 13 | Loss = 3.5106139183044434 | Accuracy = 13.793103448275861\n","Train: Epoch = 14 | Loss = 3.6622314453125 | Accuracy = 10.263929618768328\n","Val: Epoch = 14 | Loss = 3.4456381797790527 | Accuracy = 14.655172413793101\n","Train: Epoch = 15 | Loss = 3.470233917236328 | Accuracy = 15.249266862170089\n","Val: Epoch = 15 | Loss = 3.265787124633789 | Accuracy = 14.655172413793101\n","Train: Epoch = 16 | Loss = 3.462336540222168 | Accuracy = 14.95601173020528\n","Val: Epoch = 16 | Loss = 3.3105170726776123 | Accuracy = 15.517241379310345\n","Train: Epoch = 17 | Loss = 3.3659160137176514 | Accuracy = 15.835777126099707\n","Val: Epoch = 17 | Loss = 3.337374448776245 | Accuracy = 16.379310344827587\n","Train: Epoch = 18 | Loss = 3.23325777053833 | Accuracy = 14.66275659824047\n","Val: Epoch = 18 | Loss = 3.231989622116089 | Accuracy = 21.551724137931032\n","Train: Epoch = 19 | Loss = 3.325565814971924 | Accuracy = 14.076246334310852\n","Val: Epoch = 19 | Loss = 3.2355237007141113 | Accuracy = 20.689655172413794\n","Train: Epoch = 20 | Loss = 3.1882922649383545 | Accuracy = 18.7683284457478\n","Val: Epoch = 20 | Loss = 3.0659823417663574 | Accuracy = 20.689655172413794\n","Train: Epoch = 21 | Loss = 3.1751887798309326 | Accuracy = 16.715542521994134\n","Val: Epoch = 21 | Loss = 2.945957899093628 | Accuracy = 23.275862068965516\n","Train: Epoch = 22 | Loss = 3.127486228942871 | Accuracy = 17.302052785923756\n","Val: Epoch = 22 | Loss = 2.9356393814086914 | Accuracy = 29.310344827586203\n","Train: Epoch = 23 | Loss = 2.9646878242492676 | Accuracy = 23.75366568914956\n","Val: Epoch = 23 | Loss = 3.015307664871216 | Accuracy = 20.689655172413794\n","Train: Epoch = 24 | Loss = 2.893108367919922 | Accuracy = 23.46041055718475\n","Val: Epoch = 24 | Loss = 2.7338078022003174 | Accuracy = 27.586206896551722\n","Train: Epoch = 25 | Loss = 2.998816728591919 | Accuracy = 18.7683284457478\n","Val: Epoch = 25 | Loss = 3.1286003589630127 | Accuracy = 24.137931034482758\n","Train: Epoch = 26 | Loss = 2.821509599685669 | Accuracy = 26.68621700879765\n","Val: Epoch = 26 | Loss = 2.8674817085266113 | Accuracy = 28.448275862068968\n","Train: Epoch = 27 | Loss = 2.7234299182891846 | Accuracy = 25.806451612903224\n","Val: Epoch = 27 | Loss = 2.775789737701416 | Accuracy = 32.758620689655174\n","Train: Epoch = 28 | Loss = 2.6889984607696533 | Accuracy = 29.03225806451613\n","Val: Epoch = 28 | Loss = 2.795024871826172 | Accuracy = 31.896551724137932\n","Train: Epoch = 29 | Loss = 2.7186100482940674 | Accuracy = 26.68621700879765\n","Val: Epoch = 29 | Loss = 2.8222947120666504 | Accuracy = 27.586206896551722\n","Train: Epoch = 30 | Loss = 2.690345287322998 | Accuracy = 27.859237536656888\n","Val: Epoch = 30 | Loss = 2.7447092533111572 | Accuracy = 27.586206896551722\n","Train: Epoch = 31 | Loss = 2.56891131401062 | Accuracy = 29.91202346041056\n","Val: Epoch = 31 | Loss = 2.743825912475586 | Accuracy = 28.448275862068968\n","Train: Epoch = 32 | Loss = 2.5797412395477295 | Accuracy = 32.25806451612903\n","Val: Epoch = 32 | Loss = 2.780151605606079 | Accuracy = 31.896551724137932\n","Train: Epoch = 33 | Loss = 2.5282692909240723 | Accuracy = 34.01759530791789\n","Val: Epoch = 33 | Loss = 2.7273712158203125 | Accuracy = 33.62068965517241\n","Train: Epoch = 34 | Loss = 2.6128790378570557 | Accuracy = 27.859237536656888\n","Val: Epoch = 34 | Loss = 2.7345924377441406 | Accuracy = 31.03448275862069\n","Train: Epoch = 35 | Loss = 2.5441930294036865 | Accuracy = 31.085043988269796\n","Val: Epoch = 35 | Loss = 2.642075538635254 | Accuracy = 31.896551724137932\n","Train: Epoch = 36 | Loss = 2.4127602577209473 | Accuracy = 36.36363636363637\n","Val: Epoch = 36 | Loss = 2.5927987098693848 | Accuracy = 33.62068965517241\n","Train: Epoch = 37 | Loss = 2.497713804244995 | Accuracy = 33.137829912023456\n","Val: Epoch = 37 | Loss = 2.674448251724243 | Accuracy = 30.17241379310345\n","Train: Epoch = 38 | Loss = 2.5035336017608643 | Accuracy = 29.03225806451613\n","Val: Epoch = 38 | Loss = 2.6194043159484863 | Accuracy = 31.896551724137932\n","Train: Epoch = 39 | Loss = 2.5004501342773438 | Accuracy = 30.791788856304986\n","Val: Epoch = 39 | Loss = 2.560941696166992 | Accuracy = 36.206896551724135\n","Train: Epoch = 40 | Loss = 2.4738056659698486 | Accuracy = 32.25806451612903\n","Val: Epoch = 40 | Loss = 2.597674608230591 | Accuracy = 32.758620689655174\n","Train: Epoch = 41 | Loss = 2.4154162406921387 | Accuracy = 31.964809384164223\n","Val: Epoch = 41 | Loss = 2.4912338256835938 | Accuracy = 37.06896551724138\n","Train: Epoch = 42 | Loss = 2.5511341094970703 | Accuracy = 30.205278592375368\n","Val: Epoch = 42 | Loss = 2.5887184143066406 | Accuracy = 32.758620689655174\n","Train: Epoch = 43 | Loss = 2.352775812149048 | Accuracy = 34.60410557184751\n","Val: Epoch = 43 | Loss = 2.565833568572998 | Accuracy = 35.3448275862069\n","Train: Epoch = 44 | Loss = 2.3397367000579834 | Accuracy = 35.77712609970675\n","Val: Epoch = 44 | Loss = 2.5631890296936035 | Accuracy = 31.896551724137932\n","Train: Epoch = 45 | Loss = 2.331002950668335 | Accuracy = 38.41642228739003\n","Val: Epoch = 45 | Loss = 2.5639443397521973 | Accuracy = 32.758620689655174\n","Train: Epoch = 46 | Loss = 2.2608649730682373 | Accuracy = 36.950146627565985\n","Val: Epoch = 46 | Loss = 2.5840749740600586 | Accuracy = 34.48275862068966\n","Train: Epoch = 47 | Loss = 2.287602186203003 | Accuracy = 36.36363636363637\n","Val: Epoch = 47 | Loss = 2.643815040588379 | Accuracy = 31.896551724137932\n","Train: Epoch = 48 | Loss = 2.264320135116577 | Accuracy = 38.70967741935484\n","Val: Epoch = 48 | Loss = 2.554027557373047 | Accuracy = 32.758620689655174\n","Train: Epoch = 49 | Loss = 2.1990010738372803 | Accuracy = 37.82991202346041\n","Val: Epoch = 49 | Loss = 2.513615131378174 | Accuracy = 37.06896551724138\n","Train: Epoch = 50 | Loss = 2.243082046508789 | Accuracy = 37.5366568914956\n","Val: Epoch = 50 | Loss = 2.4674885272979736 | Accuracy = 37.93103448275862\n","Train: Epoch = 51 | Loss = 2.3018648624420166 | Accuracy = 32.25806451612903\n","Val: Epoch = 51 | Loss = 2.5566115379333496 | Accuracy = 34.48275862068966\n","Train: Epoch = 52 | Loss = 2.3360371589660645 | Accuracy = 34.01759530791789\n","Val: Epoch = 52 | Loss = 2.4010303020477295 | Accuracy = 39.6551724137931\n","Train: Epoch = 53 | Loss = 2.2963104248046875 | Accuracy = 38.41642228739003\n","Val: Epoch = 53 | Loss = 2.416154623031616 | Accuracy = 38.793103448275865\n","Train: Epoch = 54 | Loss = 2.180955410003662 | Accuracy = 41.935483870967744\n","Val: Epoch = 54 | Loss = 2.459038257598877 | Accuracy = 36.206896551724135\n","Train: Epoch = 55 | Loss = 2.222545623779297 | Accuracy = 41.05571847507331\n","Val: Epoch = 55 | Loss = 2.374251127243042 | Accuracy = 39.6551724137931\n","Train: Epoch = 56 | Loss = 2.1008481979370117 | Accuracy = 41.348973607038126\n","Val: Epoch = 56 | Loss = 2.442328453063965 | Accuracy = 38.793103448275865\n","Train: Epoch = 57 | Loss = 2.141172170639038 | Accuracy = 44.57478005865102\n","Val: Epoch = 57 | Loss = 2.384146213531494 | Accuracy = 47.41379310344828\n","Train: Epoch = 58 | Loss = 2.2480967044830322 | Accuracy = 38.70967741935484\n","Val: Epoch = 58 | Loss = 2.3372466564178467 | Accuracy = 45.689655172413794\n","Train: Epoch = 59 | Loss = 2.173041582107544 | Accuracy = 38.70967741935484\n","Val: Epoch = 59 | Loss = 2.4736499786376953 | Accuracy = 37.93103448275862\n","Train: Epoch = 60 | Loss = 2.1503796577453613 | Accuracy = 38.70967741935484\n","Val: Epoch = 60 | Loss = 2.382753849029541 | Accuracy = 42.241379310344826\n","Train: Epoch = 61 | Loss = 2.0322036743164062 | Accuracy = 42.22873900293255\n","Val: Epoch = 61 | Loss = 2.3136441707611084 | Accuracy = 43.103448275862064\n","Train: Epoch = 62 | Loss = 2.208972215652466 | Accuracy = 37.5366568914956\n","Val: Epoch = 62 | Loss = 2.4154391288757324 | Accuracy = 37.06896551724138\n","Train: Epoch = 63 | Loss = 2.063957691192627 | Accuracy = 40.469208211143695\n","Val: Epoch = 63 | Loss = 2.2913756370544434 | Accuracy = 41.37931034482759\n","Train: Epoch = 64 | Loss = 2.1861236095428467 | Accuracy = 39.589442815249264\n","Val: Epoch = 64 | Loss = 2.334660291671753 | Accuracy = 42.241379310344826\n","Train: Epoch = 65 | Loss = 1.9921023845672607 | Accuracy = 46.04105571847507\n","Val: Epoch = 65 | Loss = 2.3027422428131104 | Accuracy = 41.37931034482759\n","Train: Epoch = 66 | Loss = 2.1586060523986816 | Accuracy = 39.882697947214076\n","Val: Epoch = 66 | Loss = 2.409348487854004 | Accuracy = 37.93103448275862\n","Train: Epoch = 67 | Loss = 1.9451861381530762 | Accuracy = 47.214076246334315\n","Val: Epoch = 67 | Loss = 2.361078977584839 | Accuracy = 43.103448275862064\n","Train: Epoch = 68 | Loss = 1.8912757635116577 | Accuracy = 45.45454545454545\n","Val: Epoch = 68 | Loss = 2.336469888687134 | Accuracy = 42.241379310344826\n","Train: Epoch = 69 | Loss = 1.8531233072280884 | Accuracy = 47.80058651026393\n","Val: Epoch = 69 | Loss = 2.3305625915527344 | Accuracy = 36.206896551724135\n","Train: Epoch = 70 | Loss = 2.0272560119628906 | Accuracy = 41.348973607038126\n","Val: Epoch = 70 | Loss = 2.3126630783081055 | Accuracy = 40.51724137931034\n","Train: Epoch = 71 | Loss = 2.029510498046875 | Accuracy = 43.10850439882698\n","Val: Epoch = 71 | Loss = 2.3337326049804688 | Accuracy = 42.241379310344826\n","Train: Epoch = 72 | Loss = 1.8429774045944214 | Accuracy = 45.45454545454545\n","Val: Epoch = 72 | Loss = 2.332104206085205 | Accuracy = 39.6551724137931\n","Train: Epoch = 73 | Loss = 1.9271072149276733 | Accuracy = 46.04105571847507\n","Val: Epoch = 73 | Loss = 2.4043641090393066 | Accuracy = 37.06896551724138\n","Train: Epoch = 74 | Loss = 1.8385608196258545 | Accuracy = 46.04105571847507\n","Val: Epoch = 74 | Loss = 2.34981107711792 | Accuracy = 39.6551724137931\n","Train: Epoch = 75 | Loss = 1.8262815475463867 | Accuracy = 46.33431085043988\n","Val: Epoch = 75 | Loss = 2.406465530395508 | Accuracy = 43.103448275862064\n","Train: Epoch = 76 | Loss = 1.860939621925354 | Accuracy = 44.57478005865102\n","Val: Epoch = 76 | Loss = 2.3835363388061523 | Accuracy = 43.96551724137931\n","Train: Epoch = 77 | Loss = 1.7630854845046997 | Accuracy = 49.56011730205279\n","Val: Epoch = 77 | Loss = 2.342398166656494 | Accuracy = 43.96551724137931\n","Train: Epoch = 78 | Loss = 1.8998056650161743 | Accuracy = 44.868035190615835\n","Val: Epoch = 78 | Loss = 2.3387832641601562 | Accuracy = 44.827586206896555\n","Train: Epoch = 79 | Loss = 1.874778151512146 | Accuracy = 46.04105571847507\n","Val: Epoch = 79 | Loss = 2.3487210273742676 | Accuracy = 43.96551724137931\n","Train: Epoch = 80 | Loss = 1.8544459342956543 | Accuracy = 45.74780058651026\n","Val: Epoch = 80 | Loss = 2.34454345703125 | Accuracy = 45.689655172413794\n","Train: Epoch = 81 | Loss = 1.8003408908843994 | Accuracy = 50.146627565982406\n","Val: Epoch = 81 | Loss = 2.3305370807647705 | Accuracy = 44.827586206896555\n","Train: Epoch = 82 | Loss = 1.656609296798706 | Accuracy = 48.97360703812317\n","Val: Epoch = 82 | Loss = 2.3181569576263428 | Accuracy = 45.689655172413794\n","Train: Epoch = 83 | Loss = 1.771912693977356 | Accuracy = 47.50733137829912\n","Val: Epoch = 83 | Loss = 2.32218861579895 | Accuracy = 46.55172413793103\n","Train: Epoch = 84 | Loss = 1.9142512083053589 | Accuracy = 46.9208211143695\n","Val: Epoch = 84 | Loss = 2.344247817993164 | Accuracy = 45.689655172413794\n","Train: Epoch = 85 | Loss = 1.8278037309646606 | Accuracy = 44.868035190615835\n","Val: Epoch = 85 | Loss = 2.346222162246704 | Accuracy = 45.689655172413794\n","Train: Epoch = 86 | Loss = 1.8234645128250122 | Accuracy = 46.33431085043988\n","Val: Epoch = 86 | Loss = 2.337493419647217 | Accuracy = 45.689655172413794\n","Train: Epoch = 87 | Loss = 1.8356925249099731 | Accuracy = 48.97360703812317\n","Val: Epoch = 87 | Loss = 2.317293167114258 | Accuracy = 46.55172413793103\n","Train: Epoch = 88 | Loss = 1.6923480033874512 | Accuracy = 51.90615835777126\n","Val: Epoch = 88 | Loss = 2.320685386657715 | Accuracy = 46.55172413793103\n","Train: Epoch = 89 | Loss = 1.7548754215240479 | Accuracy = 51.90615835777126\n","Val: Epoch = 89 | Loss = 2.3339133262634277 | Accuracy = 47.41379310344828\n","Train: Epoch = 90 | Loss = 1.6196130514144897 | Accuracy = 55.13196480938416\n","Val: Epoch = 90 | Loss = 2.3256359100341797 | Accuracy = 47.41379310344828\n","Train: Epoch = 91 | Loss = 1.6096506118774414 | Accuracy = 53.95894428152492\n","Val: Epoch = 91 | Loss = 2.3272006511688232 | Accuracy = 47.41379310344828\n","Train: Epoch = 92 | Loss = 1.7430957555770874 | Accuracy = 49.853372434017594\n","Val: Epoch = 92 | Loss = 2.3364195823669434 | Accuracy = 46.55172413793103\n","Train: Epoch = 93 | Loss = 1.7896255254745483 | Accuracy = 47.50733137829912\n","Val: Epoch = 93 | Loss = 2.3222246170043945 | Accuracy = 45.689655172413794\n","Train: Epoch = 94 | Loss = 1.7974364757537842 | Accuracy = 48.68035190615836\n","Val: Epoch = 94 | Loss = 2.2973227500915527 | Accuracy = 44.827586206896555\n","Train: Epoch = 95 | Loss = 1.8336124420166016 | Accuracy = 47.80058651026393\n","Val: Epoch = 95 | Loss = 2.2831037044525146 | Accuracy = 44.827586206896555\n","Train: Epoch = 96 | Loss = 1.691772222518921 | Accuracy = 53.3724340175953\n","Val: Epoch = 96 | Loss = 2.2899019718170166 | Accuracy = 45.689655172413794\n","Train: Epoch = 97 | Loss = 1.9049625396728516 | Accuracy = 43.6950146627566\n","Val: Epoch = 97 | Loss = 2.3000106811523438 | Accuracy = 45.689655172413794\n","Train: Epoch = 98 | Loss = 1.719613790512085 | Accuracy = 45.16129032258064\n","Val: Epoch = 98 | Loss = 2.2937171459198 | Accuracy = 45.689655172413794\n","Train: Epoch = 99 | Loss = 1.7193293571472168 | Accuracy = 48.68035190615836\n","Val: Epoch = 99 | Loss = 2.2756361961364746 | Accuracy = 45.689655172413794\n","Train: Epoch = 100 | Loss = 1.7846145629882812 | Accuracy = 49.266862170087975\n","Val: Epoch = 100 | Loss = 2.2616190910339355 | Accuracy = 45.689655172413794\n","Train: Epoch = 101 | Loss = 1.6619937419891357 | Accuracy = 51.90615835777126\n","Val: Epoch = 101 | Loss = 2.261183977127075 | Accuracy = 46.55172413793103\n","Train: Epoch = 102 | Loss = 1.6447947025299072 | Accuracy = 53.665689149560116\n","Val: Epoch = 102 | Loss = 2.254917621612549 | Accuracy = 45.689655172413794\n","Train: Epoch = 103 | Loss = 1.6860953569412231 | Accuracy = 53.95894428152492\n","Val: Epoch = 103 | Loss = 2.260308027267456 | Accuracy = 45.689655172413794\n","Train: Epoch = 104 | Loss = 1.6133086681365967 | Accuracy = 51.02639296187683\n","Val: Epoch = 104 | Loss = 2.2501237392425537 | Accuracy = 46.55172413793103\n","Train: Epoch = 105 | Loss = 1.84857177734375 | Accuracy = 48.09384164222874\n","Val: Epoch = 105 | Loss = 2.2634365558624268 | Accuracy = 46.55172413793103\n","Train: Epoch = 106 | Loss = 1.755979061126709 | Accuracy = 51.61290322580645\n","Val: Epoch = 106 | Loss = 2.284623622894287 | Accuracy = 47.41379310344828\n","Train: Epoch = 107 | Loss = 1.8473161458969116 | Accuracy = 46.9208211143695\n","Val: Epoch = 107 | Loss = 2.2671267986297607 | Accuracy = 47.41379310344828\n","Train: Epoch = 108 | Loss = 1.5800777673721313 | Accuracy = 52.785923753665685\n","Val: Epoch = 108 | Loss = 2.2603087425231934 | Accuracy = 47.41379310344828\n","Train: Epoch = 109 | Loss = 1.7997809648513794 | Accuracy = 49.853372434017594\n","Val: Epoch = 109 | Loss = 2.2470591068267822 | Accuracy = 47.41379310344828\n","Train: Epoch = 110 | Loss = 1.660079836845398 | Accuracy = 53.0791788856305\n","Val: Epoch = 110 | Loss = 2.2634592056274414 | Accuracy = 46.55172413793103\n","Train: Epoch = 111 | Loss = 1.669145941734314 | Accuracy = 51.02639296187683\n","Val: Epoch = 111 | Loss = 2.2623636722564697 | Accuracy = 44.827586206896555\n","Train: Epoch = 112 | Loss = 1.6559499502182007 | Accuracy = 52.49266862170088\n","Val: Epoch = 112 | Loss = 2.242961883544922 | Accuracy = 45.689655172413794\n","Train: Epoch = 113 | Loss = 1.6407979726791382 | Accuracy = 53.95894428152492\n","Val: Epoch = 113 | Loss = 2.2558350563049316 | Accuracy = 46.55172413793103\n","Train: Epoch = 114 | Loss = 1.7375422716140747 | Accuracy = 48.97360703812317\n","Val: Epoch = 114 | Loss = 2.270885944366455 | Accuracy = 47.41379310344828\n","Train: Epoch = 115 | Loss = 1.6890097856521606 | Accuracy = 50.146627565982406\n","Val: Epoch = 115 | Loss = 2.2735023498535156 | Accuracy = 47.41379310344828\n","Train: Epoch = 116 | Loss = 1.7134467363357544 | Accuracy = 49.56011730205279\n","Val: Epoch = 116 | Loss = 2.2697863578796387 | Accuracy = 46.55172413793103\n","Train: Epoch = 117 | Loss = 1.5826539993286133 | Accuracy = 51.31964809384164\n","Val: Epoch = 117 | Loss = 2.2722408771514893 | Accuracy = 47.41379310344828\n","Train: Epoch = 118 | Loss = 1.7681783437728882 | Accuracy = 48.97360703812317\n","Val: Epoch = 118 | Loss = 2.2793891429901123 | Accuracy = 46.55172413793103\n","Train: Epoch = 119 | Loss = 1.7175788879394531 | Accuracy = 48.09384164222874\n","Val: Epoch = 119 | Loss = 2.265958786010742 | Accuracy = 46.55172413793103\n","Train: Epoch = 120 | Loss = 1.7646948099136353 | Accuracy = 48.68035190615836\n","Val: Epoch = 120 | Loss = 2.2434661388397217 | Accuracy = 45.689655172413794\n","Train: Epoch = 121 | Loss = 1.7359883785247803 | Accuracy = 48.09384164222874\n","Val: Epoch = 121 | Loss = 2.239377021789551 | Accuracy = 45.689655172413794\n","Train: Epoch = 122 | Loss = 1.682935118675232 | Accuracy = 52.785923753665685\n","Val: Epoch = 122 | Loss = 2.270963430404663 | Accuracy = 43.96551724137931\n","Train: Epoch = 123 | Loss = 1.8075294494628906 | Accuracy = 47.50733137829912\n","Val: Epoch = 123 | Loss = 2.285951614379883 | Accuracy = 45.689655172413794\n","Train: Epoch = 124 | Loss = 1.6672592163085938 | Accuracy = 50.146627565982406\n","Val: Epoch = 124 | Loss = 2.2901217937469482 | Accuracy = 43.96551724137931\n","Train: Epoch = 125 | Loss = 1.667342185974121 | Accuracy = 48.97360703812317\n","Val: Epoch = 125 | Loss = 2.2826199531555176 | Accuracy = 44.827586206896555\n","Train: Epoch = 126 | Loss = 1.7003886699676514 | Accuracy = 50.146627565982406\n","Val: Epoch = 126 | Loss = 2.27357816696167 | Accuracy = 45.689655172413794\n","Train: Epoch = 127 | Loss = 1.5424522161483765 | Accuracy = 54.83870967741935\n","Val: Epoch = 127 | Loss = 2.2896995544433594 | Accuracy = 45.689655172413794\n","Train: Epoch = 128 | Loss = 1.7404407262802124 | Accuracy = 51.31964809384164\n","Val: Epoch = 128 | Loss = 2.2948288917541504 | Accuracy = 45.689655172413794\n","Train: Epoch = 129 | Loss = 1.7484004497528076 | Accuracy = 48.38709677419355\n","Val: Epoch = 129 | Loss = 2.3017144203186035 | Accuracy = 43.96551724137931\n","Train: Epoch = 130 | Loss = 1.707813024520874 | Accuracy = 48.97360703812317\n","Val: Epoch = 130 | Loss = 2.3114123344421387 | Accuracy = 44.827586206896555\n","Train: Epoch = 131 | Loss = 1.6709001064300537 | Accuracy = 51.31964809384164\n","Val: Epoch = 131 | Loss = 2.2904443740844727 | Accuracy = 44.827586206896555\n","Train: Epoch = 132 | Loss = 1.6496003866195679 | Accuracy = 50.733137829912025\n","Val: Epoch = 132 | Loss = 2.253826141357422 | Accuracy = 44.827586206896555\n","Train: Epoch = 133 | Loss = 1.6258140802383423 | Accuracy = 53.665689149560116\n","Val: Epoch = 133 | Loss = 2.2521920204162598 | Accuracy = 44.827586206896555\n","Train: Epoch = 134 | Loss = 1.5285532474517822 | Accuracy = 54.83870967741935\n","Val: Epoch = 134 | Loss = 2.2662222385406494 | Accuracy = 46.55172413793103\n","Train: Epoch = 135 | Loss = 1.5349525213241577 | Accuracy = 54.83870967741935\n","Val: Epoch = 135 | Loss = 2.246654510498047 | Accuracy = 46.55172413793103\n","Train: Epoch = 136 | Loss = 1.7862275838851929 | Accuracy = 49.853372434017594\n","Val: Epoch = 136 | Loss = 2.2272443771362305 | Accuracy = 49.137931034482754\n","Train: Epoch = 137 | Loss = 1.6747019290924072 | Accuracy = 53.3724340175953\n","Val: Epoch = 137 | Loss = 2.2336039543151855 | Accuracy = 47.41379310344828\n","Train: Epoch = 138 | Loss = 1.709179162979126 | Accuracy = 49.56011730205279\n","Val: Epoch = 138 | Loss = 2.2622387409210205 | Accuracy = 45.689655172413794\n","Train: Epoch = 139 | Loss = 1.6921786069869995 | Accuracy = 51.02639296187683\n","Val: Epoch = 139 | Loss = 2.273852825164795 | Accuracy = 46.55172413793103\n","Train: Epoch = 140 | Loss = 1.7124583721160889 | Accuracy = 50.146627565982406\n","Val: Epoch = 140 | Loss = 2.276414632797241 | Accuracy = 45.689655172413794\n","Train: Epoch = 141 | Loss = 1.5904150009155273 | Accuracy = 54.54545454545454\n","Val: Epoch = 141 | Loss = 2.2646942138671875 | Accuracy = 47.41379310344828\n","Train: Epoch = 142 | Loss = 1.6776504516601562 | Accuracy = 48.68035190615836\n","Val: Epoch = 142 | Loss = 2.2478580474853516 | Accuracy = 46.55172413793103\n","Train: Epoch = 143 | Loss = 1.601478099822998 | Accuracy = 52.19941348973607\n","Val: Epoch = 143 | Loss = 2.2536799907684326 | Accuracy = 44.827586206896555\n","Train: Epoch = 144 | Loss = 1.593609094619751 | Accuracy = 53.665689149560116\n","Val: Epoch = 144 | Loss = 2.2550981044769287 | Accuracy = 45.689655172413794\n","Train: Epoch = 145 | Loss = 1.7202914953231812 | Accuracy = 48.38709677419355\n","Val: Epoch = 145 | Loss = 2.267697811126709 | Accuracy = 45.689655172413794\n","Train: Epoch = 146 | Loss = 1.730772614479065 | Accuracy = 49.56011730205279\n","Val: Epoch = 146 | Loss = 2.2833609580993652 | Accuracy = 45.689655172413794\n","Train: Epoch = 147 | Loss = 1.623402714729309 | Accuracy = 52.785923753665685\n","Val: Epoch = 147 | Loss = 2.28281307220459 | Accuracy = 44.827586206896555\n","Train: Epoch = 148 | Loss = 1.693515658378601 | Accuracy = 55.13196480938416\n","Val: Epoch = 148 | Loss = 2.2843875885009766 | Accuracy = 46.55172413793103\n","Train: Epoch = 149 | Loss = 1.5182684659957886 | Accuracy = 53.95894428152492\n","Val: Epoch = 149 | Loss = 2.2627718448638916 | Accuracy = 47.41379310344828\n","Train: Epoch = 150 | Loss = 1.7680473327636719 | Accuracy = 49.56011730205279\n","Val: Epoch = 150 | Loss = 2.2645814418792725 | Accuracy = 45.689655172413794\n","Train: Epoch = 151 | Loss = 1.4637442827224731 | Accuracy = 58.94428152492669\n","Val: Epoch = 151 | Loss = 2.2646169662475586 | Accuracy = 45.689655172413794\n","Train: Epoch = 152 | Loss = 1.5331428050994873 | Accuracy = 56.30498533724341\n","Val: Epoch = 152 | Loss = 2.2632484436035156 | Accuracy = 45.689655172413794\n","Train: Epoch = 153 | Loss = 1.692111849784851 | Accuracy = 51.61290322580645\n","Val: Epoch = 153 | Loss = 2.2621231079101562 | Accuracy = 45.689655172413794\n","Train: Epoch = 154 | Loss = 1.5820995569229126 | Accuracy = 54.83870967741935\n","Val: Epoch = 154 | Loss = 2.2617688179016113 | Accuracy = 45.689655172413794\n","Train: Epoch = 155 | Loss = 1.6629140377044678 | Accuracy = 50.733137829912025\n","Val: Epoch = 155 | Loss = 2.260143756866455 | Accuracy = 45.689655172413794\n","Train: Epoch = 156 | Loss = 1.628564715385437 | Accuracy = 50.733137829912025\n","Val: Epoch = 156 | Loss = 2.2590553760528564 | Accuracy = 45.689655172413794\n","Train: Epoch = 157 | Loss = 1.6785093545913696 | Accuracy = 50.146627565982406\n","Val: Epoch = 157 | Loss = 2.2604289054870605 | Accuracy = 45.689655172413794\n","Train: Epoch = 158 | Loss = 1.4896634817123413 | Accuracy = 58.94428152492669\n","Val: Epoch = 158 | Loss = 2.2628352642059326 | Accuracy = 45.689655172413794\n","Train: Epoch = 159 | Loss = 1.4844329357147217 | Accuracy = 55.718475073313776\n","Val: Epoch = 159 | Loss = 2.264406204223633 | Accuracy = 45.689655172413794\n","Train: Epoch = 160 | Loss = 1.6195865869522095 | Accuracy = 55.13196480938416\n","Val: Epoch = 160 | Loss = 2.265624523162842 | Accuracy = 45.689655172413794\n","Train: Epoch = 161 | Loss = 1.614740014076233 | Accuracy = 52.19941348973607\n","Val: Epoch = 161 | Loss = 2.2648019790649414 | Accuracy = 46.55172413793103\n","Train: Epoch = 162 | Loss = 1.6384589672088623 | Accuracy = 53.95894428152492\n","Val: Epoch = 162 | Loss = 2.2655510902404785 | Accuracy = 46.55172413793103\n","Train: Epoch = 163 | Loss = 1.6564520597457886 | Accuracy = 51.61290322580645\n","Val: Epoch = 163 | Loss = 2.265720844268799 | Accuracy = 45.689655172413794\n","Train: Epoch = 164 | Loss = 1.5593334436416626 | Accuracy = 50.733137829912025\n","Val: Epoch = 164 | Loss = 2.267512798309326 | Accuracy = 45.689655172413794\n","Train: Epoch = 165 | Loss = 1.6723922491073608 | Accuracy = 53.665689149560116\n","Val: Epoch = 165 | Loss = 2.267986297607422 | Accuracy = 45.689655172413794\n","Train: Epoch = 166 | Loss = 1.4408364295959473 | Accuracy = 55.42521994134897\n","Val: Epoch = 166 | Loss = 2.2694151401519775 | Accuracy = 45.689655172413794\n","Train: Epoch = 167 | Loss = 1.6501165628433228 | Accuracy = 53.665689149560116\n","Val: Epoch = 167 | Loss = 2.269096612930298 | Accuracy = 45.689655172413794\n","Train: Epoch = 168 | Loss = 1.628018856048584 | Accuracy = 53.3724340175953\n","Val: Epoch = 168 | Loss = 2.269412040710449 | Accuracy = 45.689655172413794\n","Train: Epoch = 169 | Loss = 1.7052650451660156 | Accuracy = 51.31964809384164\n","Val: Epoch = 169 | Loss = 2.2712631225585938 | Accuracy = 45.689655172413794\n","Train: Epoch = 170 | Loss = 1.6848100423812866 | Accuracy = 52.19941348973607\n","Val: Epoch = 170 | Loss = 2.2728772163391113 | Accuracy = 45.689655172413794\n","Train: Epoch = 171 | Loss = 1.5498298406600952 | Accuracy = 53.95894428152492\n","Val: Epoch = 171 | Loss = 2.2745401859283447 | Accuracy = 45.689655172413794\n","Train: Epoch = 172 | Loss = 1.4723433256149292 | Accuracy = 56.89149560117303\n","Val: Epoch = 172 | Loss = 2.275599956512451 | Accuracy = 45.689655172413794\n","Train: Epoch = 173 | Loss = 1.5011546611785889 | Accuracy = 54.54545454545454\n","Val: Epoch = 173 | Loss = 2.2762091159820557 | Accuracy = 45.689655172413794\n","Train: Epoch = 174 | Loss = 1.5652313232421875 | Accuracy = 51.90615835777126\n","Val: Epoch = 174 | Loss = 2.2747697830200195 | Accuracy = 45.689655172413794\n","Train: Epoch = 175 | Loss = 1.5873762369155884 | Accuracy = 54.252199413489734\n","Val: Epoch = 175 | Loss = 2.274082899093628 | Accuracy = 45.689655172413794\n","Train: Epoch = 176 | Loss = 1.5308605432510376 | Accuracy = 55.13196480938416\n","Val: Epoch = 176 | Loss = 2.271191358566284 | Accuracy = 45.689655172413794\n","Train: Epoch = 177 | Loss = 1.488099455833435 | Accuracy = 55.42521994134897\n","Val: Epoch = 177 | Loss = 2.26992130279541 | Accuracy = 45.689655172413794\n","Train: Epoch = 178 | Loss = 1.6247576475143433 | Accuracy = 51.61290322580645\n","Val: Epoch = 178 | Loss = 2.270669460296631 | Accuracy = 45.689655172413794\n","Train: Epoch = 179 | Loss = 1.6399942636489868 | Accuracy = 51.31964809384164\n","Val: Epoch = 179 | Loss = 2.2686686515808105 | Accuracy = 45.689655172413794\n","Train: Epoch = 180 | Loss = 1.685502052307129 | Accuracy = 48.97360703812317\n","Val: Epoch = 180 | Loss = 2.268385887145996 | Accuracy = 45.689655172413794\n","Train: Epoch = 181 | Loss = 1.631909728050232 | Accuracy = 55.13196480938416\n","Val: Epoch = 181 | Loss = 2.2696280479431152 | Accuracy = 45.689655172413794\n","Train: Epoch = 182 | Loss = 1.5263475179672241 | Accuracy = 57.77126099706745\n","Val: Epoch = 182 | Loss = 2.27223539352417 | Accuracy = 44.827586206896555\n","Train: Epoch = 183 | Loss = 1.5808790922164917 | Accuracy = 53.0791788856305\n","Val: Epoch = 183 | Loss = 2.2738900184631348 | Accuracy = 45.689655172413794\n","Train: Epoch = 184 | Loss = 1.6541211605072021 | Accuracy = 52.49266862170088\n","Val: Epoch = 184 | Loss = 2.274118185043335 | Accuracy = 45.689655172413794\n","Train: Epoch = 185 | Loss = 1.5256518125534058 | Accuracy = 56.30498533724341\n","Val: Epoch = 185 | Loss = 2.274219036102295 | Accuracy = 45.689655172413794\n","Train: Epoch = 186 | Loss = 1.5773265361785889 | Accuracy = 52.49266862170088\n","Val: Epoch = 186 | Loss = 2.274057388305664 | Accuracy = 45.689655172413794\n","Train: Epoch = 187 | Loss = 1.5154212713241577 | Accuracy = 53.95894428152492\n","Val: Epoch = 187 | Loss = 2.274463176727295 | Accuracy = 45.689655172413794\n","Train: Epoch = 188 | Loss = 1.5578627586364746 | Accuracy = 50.733137829912025\n","Val: Epoch = 188 | Loss = 2.2752139568328857 | Accuracy = 45.689655172413794\n","Train: Epoch = 189 | Loss = 1.625078558921814 | Accuracy = 55.718475073313776\n","Val: Epoch = 189 | Loss = 2.275451183319092 | Accuracy = 45.689655172413794\n","Train: Epoch = 190 | Loss = 1.6698896884918213 | Accuracy = 51.31964809384164\n","Val: Epoch = 190 | Loss = 2.2737796306610107 | Accuracy = 45.689655172413794\n","Train: Epoch = 191 | Loss = 1.6292288303375244 | Accuracy = 53.0791788856305\n","Val: Epoch = 191 | Loss = 2.2731263637542725 | Accuracy = 45.689655172413794\n","Train: Epoch = 192 | Loss = 1.6170581579208374 | Accuracy = 52.49266862170088\n","Val: Epoch = 192 | Loss = 2.272477149963379 | Accuracy = 45.689655172413794\n","Train: Epoch = 193 | Loss = 1.6603068113327026 | Accuracy = 51.31964809384164\n","Val: Epoch = 193 | Loss = 2.2725658416748047 | Accuracy = 45.689655172413794\n","Train: Epoch = 194 | Loss = 1.4993807077407837 | Accuracy = 54.54545454545454\n","Val: Epoch = 194 | Loss = 2.273064374923706 | Accuracy = 44.827586206896555\n","Train: Epoch = 195 | Loss = 1.6066560745239258 | Accuracy = 49.853372434017594\n","Val: Epoch = 195 | Loss = 2.2744626998901367 | Accuracy = 44.827586206896555\n","Train: Epoch = 196 | Loss = 1.7364991903305054 | Accuracy = 51.61290322580645\n","Val: Epoch = 196 | Loss = 2.2765016555786133 | Accuracy = 44.827586206896555\n","Train: Epoch = 197 | Loss = 1.6584700345993042 | Accuracy = 51.31964809384164\n","Val: Epoch = 197 | Loss = 2.2757623195648193 | Accuracy = 44.827586206896555\n","Train: Epoch = 198 | Loss = 1.6410468816757202 | Accuracy = 51.61290322580645\n","Val: Epoch = 198 | Loss = 2.275421380996704 | Accuracy = 44.827586206896555\n","Train: Epoch = 199 | Loss = 1.4886908531188965 | Accuracy = 56.30498533724341\n","Val: Epoch = 199 | Loss = 2.2761144638061523 | Accuracy = 44.827586206896555\n","Train: Epoch = 200 | Loss = 1.6168220043182373 | Accuracy = 51.61290322580645\n","Val: Epoch = 200 | Loss = 2.2759292125701904 | Accuracy = 44.827586206896555\n","Train: Epoch = 201 | Loss = 1.512802004814148 | Accuracy = 57.77126099706745\n","Val: Epoch = 201 | Loss = 2.275268793106079 | Accuracy = 44.827586206896555\n","Train: Epoch = 202 | Loss = 1.630096435546875 | Accuracy = 52.785923753665685\n","Val: Epoch = 202 | Loss = 2.2741007804870605 | Accuracy = 44.827586206896555\n","Train: Epoch = 203 | Loss = 1.5440473556518555 | Accuracy = 56.598240469208214\n","Val: Epoch = 203 | Loss = 2.2735280990600586 | Accuracy = 44.827586206896555\n","Train: Epoch = 204 | Loss = 1.589407205581665 | Accuracy = 56.30498533724341\n","Val: Epoch = 204 | Loss = 2.2733869552612305 | Accuracy = 45.689655172413794\n","Train: Epoch = 205 | Loss = 1.5564559698104858 | Accuracy = 53.95894428152492\n","Val: Epoch = 205 | Loss = 2.273632764816284 | Accuracy = 45.689655172413794\n","Train: Epoch = 206 | Loss = 1.5092047452926636 | Accuracy = 53.665689149560116\n","Val: Epoch = 206 | Loss = 2.2733659744262695 | Accuracy = 45.689655172413794\n","Train: Epoch = 207 | Loss = 1.7140768766403198 | Accuracy = 48.38709677419355\n","Val: Epoch = 207 | Loss = 2.2714297771453857 | Accuracy = 45.689655172413794\n","Train: Epoch = 208 | Loss = 1.6219091415405273 | Accuracy = 51.61290322580645\n","Val: Epoch = 208 | Loss = 2.2702972888946533 | Accuracy = 45.689655172413794\n","Train: Epoch = 209 | Loss = 1.5795722007751465 | Accuracy = 51.90615835777126\n","Val: Epoch = 209 | Loss = 2.269469976425171 | Accuracy = 45.689655172413794\n","Train: Epoch = 210 | Loss = 1.5156397819519043 | Accuracy = 55.13196480938416\n","Val: Epoch = 210 | Loss = 2.2693722248077393 | Accuracy = 45.689655172413794\n","Train: Epoch = 211 | Loss = 1.6625971794128418 | Accuracy = 50.43988269794721\n","Val: Epoch = 211 | Loss = 2.2694506645202637 | Accuracy = 44.827586206896555\n","Train: Epoch = 212 | Loss = 1.5754417181015015 | Accuracy = 55.718475073313776\n","Val: Epoch = 212 | Loss = 2.2696948051452637 | Accuracy = 44.827586206896555\n","Train: Epoch = 213 | Loss = 1.6944589614868164 | Accuracy = 51.90615835777126\n","Val: Epoch = 213 | Loss = 2.269975185394287 | Accuracy = 44.827586206896555\n","Train: Epoch = 214 | Loss = 1.5034364461898804 | Accuracy = 54.252199413489734\n","Val: Epoch = 214 | Loss = 2.2708754539489746 | Accuracy = 44.827586206896555\n","Train: Epoch = 215 | Loss = 1.6459834575653076 | Accuracy = 52.19941348973607\n","Val: Epoch = 215 | Loss = 2.2711668014526367 | Accuracy = 44.827586206896555\n","Train: Epoch = 216 | Loss = 1.5559176206588745 | Accuracy = 51.61290322580645\n","Val: Epoch = 216 | Loss = 2.2719202041625977 | Accuracy = 44.827586206896555\n","Train: Epoch = 217 | Loss = 1.5468757152557373 | Accuracy = 52.19941348973607\n","Val: Epoch = 217 | Loss = 2.272362470626831 | Accuracy = 44.827586206896555\n","Train: Epoch = 218 | Loss = 1.6655573844909668 | Accuracy = 50.733137829912025\n","Val: Epoch = 218 | Loss = 2.2728359699249268 | Accuracy = 44.827586206896555\n","Train: Epoch = 219 | Loss = 1.6432743072509766 | Accuracy = 52.785923753665685\n","Val: Epoch = 219 | Loss = 2.273027181625366 | Accuracy = 44.827586206896555\n","Train: Epoch = 220 | Loss = 1.746870756149292 | Accuracy = 49.56011730205279\n","Val: Epoch = 220 | Loss = 2.273407459259033 | Accuracy = 44.827586206896555\n","Train: Epoch = 221 | Loss = 1.6084803342819214 | Accuracy = 54.252199413489734\n","Val: Epoch = 221 | Loss = 2.271864891052246 | Accuracy = 44.827586206896555\n","Train: Epoch = 222 | Loss = 1.4807875156402588 | Accuracy = 55.13196480938416\n","Val: Epoch = 222 | Loss = 2.2703158855438232 | Accuracy = 45.689655172413794\n","Train: Epoch = 223 | Loss = 1.625689148902893 | Accuracy = 51.61290322580645\n","Val: Epoch = 223 | Loss = 2.2706925868988037 | Accuracy = 45.689655172413794\n","Train: Epoch = 224 | Loss = 1.6374244689941406 | Accuracy = 50.43988269794721\n","Val: Epoch = 224 | Loss = 2.271803379058838 | Accuracy = 45.689655172413794\n","Train: Epoch = 225 | Loss = 1.5724830627441406 | Accuracy = 56.01173020527859\n","Val: Epoch = 225 | Loss = 2.269719123840332 | Accuracy = 45.689655172413794\n","Train: Epoch = 226 | Loss = 1.4776171445846558 | Accuracy = 55.718475073313776\n","Val: Epoch = 226 | Loss = 2.269313335418701 | Accuracy = 46.55172413793103\n","Train: Epoch = 227 | Loss = 1.4353148937225342 | Accuracy = 57.77126099706745\n","Val: Epoch = 227 | Loss = 2.2712883949279785 | Accuracy = 46.55172413793103\n","Train: Epoch = 228 | Loss = 1.5425739288330078 | Accuracy = 53.95894428152492\n","Val: Epoch = 228 | Loss = 2.2720303535461426 | Accuracy = 46.55172413793103\n","Train: Epoch = 229 | Loss = 1.5851534605026245 | Accuracy = 56.01173020527859\n","Val: Epoch = 229 | Loss = 2.272106170654297 | Accuracy = 46.55172413793103\n","Train: Epoch = 230 | Loss = 1.4701147079467773 | Accuracy = 55.42521994134897\n","Val: Epoch = 230 | Loss = 2.271745443344116 | Accuracy = 46.55172413793103\n","Train: Epoch = 231 | Loss = 1.4681910276412964 | Accuracy = 58.35777126099707\n","Val: Epoch = 231 | Loss = 2.2715821266174316 | Accuracy = 46.55172413793103\n","Train: Epoch = 232 | Loss = 1.5608125925064087 | Accuracy = 51.90615835777126\n","Val: Epoch = 232 | Loss = 2.269989490509033 | Accuracy = 46.55172413793103\n","Train: Epoch = 233 | Loss = 1.4932012557983398 | Accuracy = 56.01173020527859\n","Val: Epoch = 233 | Loss = 2.269362688064575 | Accuracy = 45.689655172413794\n","Train: Epoch = 234 | Loss = 1.5307607650756836 | Accuracy = 55.42521994134897\n","Val: Epoch = 234 | Loss = 2.2706942558288574 | Accuracy = 45.689655172413794\n","Train: Epoch = 235 | Loss = 1.6882284879684448 | Accuracy = 49.266862170087975\n","Val: Epoch = 235 | Loss = 2.2727909088134766 | Accuracy = 45.689655172413794\n","Train: Epoch = 236 | Loss = 1.7290414571762085 | Accuracy = 52.19941348973607\n","Val: Epoch = 236 | Loss = 2.2734458446502686 | Accuracy = 45.689655172413794\n","Train: Epoch = 237 | Loss = 1.4818778038024902 | Accuracy = 57.478005865102645\n","Val: Epoch = 237 | Loss = 2.2735519409179688 | Accuracy = 45.689655172413794\n","Train: Epoch = 238 | Loss = 1.4984190464019775 | Accuracy = 57.77126099706745\n","Val: Epoch = 238 | Loss = 2.2736990451812744 | Accuracy = 45.689655172413794\n","Train: Epoch = 239 | Loss = 1.4432882070541382 | Accuracy = 56.598240469208214\n","Val: Epoch = 239 | Loss = 2.2726616859436035 | Accuracy = 45.689655172413794\n","Train: Epoch = 240 | Loss = 1.5319207906723022 | Accuracy = 56.01173020527859\n","Val: Epoch = 240 | Loss = 2.2725276947021484 | Accuracy = 45.689655172413794\n","Train: Epoch = 241 | Loss = 1.6548823118209839 | Accuracy = 50.733137829912025\n","Val: Epoch = 241 | Loss = 2.2739152908325195 | Accuracy = 45.689655172413794\n","Train: Epoch = 242 | Loss = 1.6090571880340576 | Accuracy = 49.56011730205279\n","Val: Epoch = 242 | Loss = 2.274355173110962 | Accuracy = 45.689655172413794\n","Train: Epoch = 243 | Loss = 1.635785460472107 | Accuracy = 50.733137829912025\n","Val: Epoch = 243 | Loss = 2.2752652168273926 | Accuracy = 45.689655172413794\n","Train: Epoch = 244 | Loss = 1.6749464273452759 | Accuracy = 47.50733137829912\n","Val: Epoch = 244 | Loss = 2.275866985321045 | Accuracy = 45.689655172413794\n","Train: Epoch = 245 | Loss = 1.6850738525390625 | Accuracy = 51.90615835777126\n","Val: Epoch = 245 | Loss = 2.276054620742798 | Accuracy = 45.689655172413794\n","Train: Epoch = 246 | Loss = 1.6853930950164795 | Accuracy = 50.43988269794721\n","Val: Epoch = 246 | Loss = 2.2789571285247803 | Accuracy = 45.689655172413794\n","Train: Epoch = 247 | Loss = 1.685901403427124 | Accuracy = 53.0791788856305\n","Val: Epoch = 247 | Loss = 2.2809805870056152 | Accuracy = 45.689655172413794\n","Train: Epoch = 248 | Loss = 1.6221613883972168 | Accuracy = 56.89149560117303\n","Val: Epoch = 248 | Loss = 2.281935691833496 | Accuracy = 44.827586206896555\n","Train: Epoch = 249 | Loss = 1.5542960166931152 | Accuracy = 53.0791788856305\n","Val: Epoch = 249 | Loss = 2.28220796585083 | Accuracy = 44.827586206896555\n","Train: Epoch = 250 | Loss = 1.4919267892837524 | Accuracy = 57.18475073313783\n","Val: Epoch = 250 | Loss = 2.28263258934021 | Accuracy = 44.827586206896555\n","Train: Epoch = 251 | Loss = 1.5374053716659546 | Accuracy = 56.598240469208214\n","Val: Epoch = 251 | Loss = 2.2849669456481934 | Accuracy = 44.827586206896555\n","Train: Epoch = 252 | Loss = 1.4133962392807007 | Accuracy = 60.70381231671554\n","Val: Epoch = 252 | Loss = 2.2866878509521484 | Accuracy = 44.827586206896555\n","Train: Epoch = 253 | Loss = 1.4177643060684204 | Accuracy = 56.598240469208214\n","Val: Epoch = 253 | Loss = 2.2861881256103516 | Accuracy = 44.827586206896555\n","Train: Epoch = 254 | Loss = 1.4954824447631836 | Accuracy = 59.530791788856305\n","Val: Epoch = 254 | Loss = 2.2835426330566406 | Accuracy = 44.827586206896555\n","Train: Epoch = 255 | Loss = 1.5826281309127808 | Accuracy = 53.3724340175953\n","Val: Epoch = 255 | Loss = 2.281630277633667 | Accuracy = 44.827586206896555\n","Train: Epoch = 256 | Loss = 1.5209647417068481 | Accuracy = 56.01173020527859\n","Val: Epoch = 256 | Loss = 2.2821290493011475 | Accuracy = 44.827586206896555\n","Train: Epoch = 257 | Loss = 1.5559909343719482 | Accuracy = 56.30498533724341\n","Val: Epoch = 257 | Loss = 2.282071828842163 | Accuracy = 44.827586206896555\n","Train: Epoch = 258 | Loss = 1.5572580099105835 | Accuracy = 53.95894428152492\n","Val: Epoch = 258 | Loss = 2.2827115058898926 | Accuracy = 44.827586206896555\n","Train: Epoch = 259 | Loss = 1.7433176040649414 | Accuracy = 51.90615835777126\n","Val: Epoch = 259 | Loss = 2.28256893157959 | Accuracy = 44.827586206896555\n","Train: Epoch = 260 | Loss = 1.5545153617858887 | Accuracy = 54.252199413489734\n","Val: Epoch = 260 | Loss = 2.2838282585144043 | Accuracy = 44.827586206896555\n","Train: Epoch = 261 | Loss = 1.513163685798645 | Accuracy = 55.42521994134897\n","Val: Epoch = 261 | Loss = 2.2827224731445312 | Accuracy = 44.827586206896555\n","Train: Epoch = 262 | Loss = 1.5926241874694824 | Accuracy = 56.30498533724341\n","Val: Epoch = 262 | Loss = 2.2828948497772217 | Accuracy = 44.827586206896555\n","Train: Epoch = 263 | Loss = 1.7221273183822632 | Accuracy = 53.665689149560116\n","Val: Epoch = 263 | Loss = 2.281648635864258 | Accuracy = 44.827586206896555\n","Train: Epoch = 264 | Loss = 1.4639616012573242 | Accuracy = 54.83870967741935\n","Val: Epoch = 264 | Loss = 2.278859853744507 | Accuracy = 45.689655172413794\n","Train: Epoch = 265 | Loss = 1.6203547716140747 | Accuracy = 49.266862170087975\n","Val: Epoch = 265 | Loss = 2.2793216705322266 | Accuracy = 45.689655172413794\n","Train: Epoch = 266 | Loss = 1.5712305307388306 | Accuracy = 56.01173020527859\n","Val: Epoch = 266 | Loss = 2.278806209564209 | Accuracy = 44.827586206896555\n","Train: Epoch = 267 | Loss = 1.5677108764648438 | Accuracy = 53.665689149560116\n","Val: Epoch = 267 | Loss = 2.277822732925415 | Accuracy = 44.827586206896555\n","Train: Epoch = 268 | Loss = 1.4133857488632202 | Accuracy = 54.252199413489734\n","Val: Epoch = 268 | Loss = 2.2780773639678955 | Accuracy = 44.827586206896555\n","Train: Epoch = 269 | Loss = 1.7264471054077148 | Accuracy = 50.43988269794721\n","Val: Epoch = 269 | Loss = 2.2779388427734375 | Accuracy = 45.689655172413794\n","Train: Epoch = 270 | Loss = 1.6898642778396606 | Accuracy = 51.61290322580645\n","Val: Epoch = 270 | Loss = 2.278898000717163 | Accuracy = 44.827586206896555\n","Train: Epoch = 271 | Loss = 1.6345990896224976 | Accuracy = 54.252199413489734\n","Val: Epoch = 271 | Loss = 2.276724100112915 | Accuracy = 45.689655172413794\n","Train: Epoch = 272 | Loss = 1.5860391855239868 | Accuracy = 56.89149560117303\n","Val: Epoch = 272 | Loss = 2.2758007049560547 | Accuracy = 44.827586206896555\n","Train: Epoch = 273 | Loss = 1.5474543571472168 | Accuracy = 53.665689149560116\n","Val: Epoch = 273 | Loss = 2.274183988571167 | Accuracy = 44.827586206896555\n","Train: Epoch = 274 | Loss = 1.5938962697982788 | Accuracy = 56.598240469208214\n","Val: Epoch = 274 | Loss = 2.2723522186279297 | Accuracy = 44.827586206896555\n","Train: Epoch = 275 | Loss = 1.6203044652938843 | Accuracy = 53.665689149560116\n","Val: Epoch = 275 | Loss = 2.272770404815674 | Accuracy = 44.827586206896555\n","Train: Epoch = 276 | Loss = 1.54277765750885 | Accuracy = 51.90615835777126\n","Val: Epoch = 276 | Loss = 2.27347469329834 | Accuracy = 45.689655172413794\n","Train: Epoch = 277 | Loss = 1.5641523599624634 | Accuracy = 53.3724340175953\n","Val: Epoch = 277 | Loss = 2.2746052742004395 | Accuracy = 45.689655172413794\n","Train: Epoch = 278 | Loss = 1.4773095846176147 | Accuracy = 53.3724340175953\n","Val: Epoch = 278 | Loss = 2.2741665840148926 | Accuracy = 45.689655172413794\n","Train: Epoch = 279 | Loss = 1.6319563388824463 | Accuracy = 52.49266862170088\n","Val: Epoch = 279 | Loss = 2.2718348503112793 | Accuracy = 45.689655172413794\n","Train: Epoch = 280 | Loss = 1.5507400035858154 | Accuracy = 54.252199413489734\n","Val: Epoch = 280 | Loss = 2.272000551223755 | Accuracy = 45.689655172413794\n","Train: Epoch = 281 | Loss = 1.597212553024292 | Accuracy = 53.95894428152492\n","Val: Epoch = 281 | Loss = 2.2724907398223877 | Accuracy = 46.55172413793103\n","Train: Epoch = 282 | Loss = 1.5865854024887085 | Accuracy = 55.42521994134897\n","Val: Epoch = 282 | Loss = 2.2730796337127686 | Accuracy = 46.55172413793103\n","Train: Epoch = 283 | Loss = 1.5835543870925903 | Accuracy = 55.718475073313776\n","Val: Epoch = 283 | Loss = 2.273181676864624 | Accuracy = 46.55172413793103\n","Train: Epoch = 284 | Loss = 1.6385772228240967 | Accuracy = 53.0791788856305\n","Val: Epoch = 284 | Loss = 2.27534818649292 | Accuracy = 46.55172413793103\n","Train: Epoch = 285 | Loss = 1.4688981771469116 | Accuracy = 56.89149560117303\n","Val: Epoch = 285 | Loss = 2.2764148712158203 | Accuracy = 45.689655172413794\n","Train: Epoch = 286 | Loss = 1.5616518259048462 | Accuracy = 55.718475073313776\n","Val: Epoch = 286 | Loss = 2.275084972381592 | Accuracy = 45.689655172413794\n","Train: Epoch = 287 | Loss = 1.5997289419174194 | Accuracy = 52.19941348973607\n","Val: Epoch = 287 | Loss = 2.2745468616485596 | Accuracy = 45.689655172413794\n","Train: Epoch = 288 | Loss = 1.666802167892456 | Accuracy = 55.42521994134897\n","Val: Epoch = 288 | Loss = 2.2749950885772705 | Accuracy = 45.689655172413794\n","Train: Epoch = 289 | Loss = 1.585610032081604 | Accuracy = 52.785923753665685\n","Val: Epoch = 289 | Loss = 2.2722342014312744 | Accuracy = 45.689655172413794\n","Train: Epoch = 290 | Loss = 1.5641916990280151 | Accuracy = 55.718475073313776\n","Val: Epoch = 290 | Loss = 2.2714545726776123 | Accuracy = 45.689655172413794\n","Train: Epoch = 291 | Loss = 1.558247685432434 | Accuracy = 54.252199413489734\n","Val: Epoch = 291 | Loss = 2.273305892944336 | Accuracy = 45.689655172413794\n","Train: Epoch = 292 | Loss = 1.4884841442108154 | Accuracy = 56.01173020527859\n","Val: Epoch = 292 | Loss = 2.27323842048645 | Accuracy = 45.689655172413794\n","Train: Epoch = 293 | Loss = 1.6052700281143188 | Accuracy = 50.43988269794721\n","Val: Epoch = 293 | Loss = 2.2743077278137207 | Accuracy = 46.55172413793103\n","Train: Epoch = 294 | Loss = 1.580174446105957 | Accuracy = 54.83870967741935\n","Val: Epoch = 294 | Loss = 2.2760257720947266 | Accuracy = 44.827586206896555\n","Train: Epoch = 295 | Loss = 1.646153450012207 | Accuracy = 53.0791788856305\n","Val: Epoch = 295 | Loss = 2.2757441997528076 | Accuracy = 45.689655172413794\n","Train: Epoch = 296 | Loss = 1.5325145721435547 | Accuracy = 53.3724340175953\n","Val: Epoch = 296 | Loss = 2.272181510925293 | Accuracy = 45.689655172413794\n","Train: Epoch = 297 | Loss = 1.6546279191970825 | Accuracy = 54.252199413489734\n","Val: Epoch = 297 | Loss = 2.2714436054229736 | Accuracy = 45.689655172413794\n","Train: Epoch = 298 | Loss = 1.4331920146942139 | Accuracy = 56.01173020527859\n","Val: Epoch = 298 | Loss = 2.2695579528808594 | Accuracy = 46.55172413793103\n","Train: Epoch = 299 | Loss = 1.558567762374878 | Accuracy = 54.54545454545454\n","Val: Epoch = 299 | Loss = 2.2684688568115234 | Accuracy = 44.827586206896555\n","Train: Epoch = 300 | Loss = 1.6131728887557983 | Accuracy = 54.83870967741935\n","Val: Epoch = 300 | Loss = 2.2687456607818604 | Accuracy = 45.689655172413794\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kmFeugoykMra","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597684830620,"user_tz":-120,"elapsed":2148,"user":{"displayName":"GMG MLDL","photoUrl":"","userId":"07893454363778055087"}},"outputId":"4e8f1fe7-53a5-4121-e737-ef90184f1b96"},"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","print('All changes made in this colab session should now be visible in Drive.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["All changes made in this colab session should now be visible in Drive.\n"],"name":"stdout"}]}]}